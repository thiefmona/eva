# 面试

## [常用数据结构](https://blog.csdn.net/Thera_qing/article/details/114169704)

### map

- hashmap

  hashmap底层是数组+链表实现的 1.8之后引入了红黑树。通过计算哈希值 找到数组对应的位置 如果已存在元素 则加到这个位置的链表上 在java8之后 如果链表的长度大于8 则会转化成红黑树 在转化为红黑树之前 会进行判断 如果数组的长度小于64 则会进行数组扩容 而不是转化成红黑树
  
	- 底层实现原理

	- 扩容机制

	- 16位异或计算哈希

	- 使用红黑树

- concurrentmap

	- 如何保证线程安全

	  1、ConcurrentHashMap 在 JDK 1.7 中使用的数组 加 链表的结构，其中数组分为两类，大树组 Segment 和 小数组 HashEntry，而加锁是通过给 Segment 添加 ReentrantLock 重入锁来保证线程安全的。 
	  2、ConcurrentHashMap 在 JDK1.8 中使用的是数组 加 链表 加 红黑树的方式实现， 它是通过 CAS 或者 synchronized 来保证线程安全的，并且缩小了锁的粒度，查询 性能也更高。
	  
		- segment加锁

		- 1.8 CAS和synchronized

		  相比于分段锁，这种实现方式有以下优点：
		  减少锁竞争：由于每个桶内部采用了synchronized来实现同步，不同的线程可以同时访问不同的桶，从而减少了锁竞争。
		  
		  更高的并发度：由于不再受限于固定数量的段，ConcurrentHashMap 可以根据需要动态调整大小，并支持更高的并发度。
		  
		  更好的扩展性：由于不再需要维护多个段的锁，因此在扩展时可以更容易地添加或删除桶，而不需要重构整个数据结构。
		  
		  更好的性能：使用CAS操作替代了分段锁，避免了分段锁中的自旋等待开销，提高了并发性能。
		  
		  
- hashtable

### list

- ArrayList扩容 1.5倍长度

### set

### 介绍一下线程安全的集合

## jdk1.8

### lambda表达式

### stream流

### 函数式编程

### 方法引用、构造引用(::)

### 新的时间日期类

## 多线程

### 线程的创建方式

- 继承Thread类

- 实现runnable接口

- 实现callable接口

### [synchronized关键字解释](https://zhuanlan.zhihu.com/p/435839659)

- [解释](https://blog.csdn.net/m0_58407085/article/details/130226265)

### 线程池

- [核心参数](https://baijiahao.baidu.com/s?id=1710260027497542286&wfr=spider&for=pc)

	- 如何设置核心参数

	- 拒绝策略配置

- 工作流程

- 用的哪个线程池？为什么不用jdk自带的

	- fixed，single

		- 阻塞队列

	- cached，schedule

		- 最大线程数

- [两种提交方式区别](https://blog.csdn.net/Mr_Zhang____/article/details/121866974)

- 状态

  RUNNING 
   该状态✁线程池会➓收新任务，并处理阻塞队列中✁任务; 
   调用线程池✁ shutdown()方法，可以切换到 SHUTDOWN 状态; 
   调用线程池✁ shutdownNow()方法，可以切换到 STOP 状态; 
  SHUTDOWN 
   该状态✁线程池不会➓收新任务，但会处理阻塞队列中✁任务； 
   队列为空，并且线程池中执行✁任务也为空,进入 TIDYING 状态; 
  STOP 
   该状态✁线程不会➓收新任务，也不会处理阻塞队列中✁任务，而且会中断正在运 
  行✁任务； 线程池中执行✁任务为空,进入 TIDYING 状态; 
  TIDYING 
   该状态表明所有✁任务已经运行终止，记录✁任务数量为 0。 
   terminated()执行完毕，进入 TERMINATED 状态 
  TERMINATED 
   该状态表示线程池彻底终止
  
### 锁

- 乐观锁

- 悲观锁

### [compeleteableFuture](https://blog.csdn.net/weixin_44337445/article/details/121710515)

### [lock类有哪些方法和参数](https://blog.csdn.net/vincent_wen0766/article/details/108538248?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-108538248-blog-104495669.235%5Ev38%5Epc_relevant_anti_vip_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-108538248-blog-104495669.235%5Ev38%5Epc_relevant_anti_vip_base&utm_relevant_index=5)

### [ReentrantLock详解](https://blog.csdn.net/ZSA222/article/details/123433746)

### [乐观锁和悲观锁的区别](https://blog.csdn.net/striveb/article/details/84826921)

### [线程间通信的几种方式](https://wenku.baidu.com/view/0b2e0715f6335a8102d276a20029bd64783e6225.html?_wkts_=1691904030144&bdQuery=线程通信的几种方法+java)

### [wait和sleep的区别](https://zhuanlan.zhihu.com/p/471109617)

### volatile关键字的作用

1. 可以保证在多线程环境下共享变量的可见性。 
2. 通过增加内存屏障防止多个指令之间的重排序。

- 和synchronized的区别

  
  1.volatile仅能使用在变量级别；
  synchronized则可以使用在变量、方法、和类级别的
  
  2.volatile仅能实现变量的修改可见性，并不能保证原子性；
     synchronized则可以保证变量的修改可见性和原子性
  
  3.volatile不会造成线程的阻塞；
  synchronized可能会造成线程的阻塞。
  
  4.volatile标记的变量不会被编译器优化；
  synchronized标记的变量可以被编译器优化
  
### ThreadLocal

1. ThreadLocal 是一种线程隔离机制，它提供了多线程环境下对于共享变量访问的安 
全性。 
2. 在多线程访问共享变量的场景中（出现下面第一个图），一般的解决办法是对共享 
变量加锁（出现下面第二个图），从而保证在同一时刻只有一个线程能够对共享变 
量进行更新，并且基于 Happens-Before 规则里面的监视器锁规则，又保证了数据 
修改后对其他线程的可见性。3. 但是加锁会带来性能的下降，所以 ThreadLocal 用了一种空间换时间的设计思想， 
也就是说在每个线程里面，都有一个容器来存储共享变量的副本，然后每个线程只 对自己的变量副本来做更新操作，这样既解决了线程安全问题，又避免了多线程竞 争加锁的开销。 
4. ThreadLocal 的具体实现原理是，在 Thread 类里面有一个成员变量 
ThreadLocalMap，它专门来存储当前线程的共享变量副本，后续这个线程对于共 
享变量的操作，都是从这个 ThreadLocalMap 里面进行变更，不会影响全局共享变量的值

### lock和synchronized区别

1. 从功能角度来看，Lock 和 Synchronized 都是 Java 中用来解决线程安全问题的工 
具。 
2. 从特性来看， 
a. Synchronized 是 Java 中的同步关键字，Lock 是 J.U.C 包中提供的接口，这 
个接口有很多实现类，其中就包括 ReentrantLock 重入锁 
b. Synchronized 可以通过两种方式来控制锁的粒度，（贴图）一种是把 synchronized 关键字修饰在方法层面， 
另一种是修饰在代码块上，并且我们可以通过 Synchronized 加锁对象的声明周期来控 
制锁的作用范围，比如锁对象是静态对象或者类对象，那么这个锁就是全局锁。 
如果锁对象是普通实例对象，那这个锁的范围取决于这个实例的声明周期。 
Lock 锁的粒度是通过它里面提供的 lock()和 unlock()方法决定的（贴图），包裹在这 
两个方法之间的代码能够保证线程安全性。而锁的作用域取决于 Lock 实例的生命周期。c. Lock 比 Synchronized 的灵活性更高，Lock 可以自主决定什么时候加锁，什 
么时候释放锁，只需要调用 lock()和 unlock()这两个方法就行，同时 Lock 还 
提供了非阻塞的竞争锁方法 tryLock()方法，这个方法通过返回 true/false 来 
告诉当前线程是否已经有其他线程正在使用锁。 
Synchronized 由于是关键字，所以它无法实现非阻塞竞争锁的方法，另外， 
Synchronized 锁的释放是被动的，就是当 Synchronized 同步代码块执行完以后或者 
代码出现异常时才会释放。 
d. Lock 提供了公平锁和非公平锁的机制，公平锁是指线程竞争锁资源时，如果 
已经有其他线程正在排队等待锁释放，那么当前竞争锁资源的线程无法插队。 
而非公平锁，就是不管是否有线程在排队等待锁，它都会尝试去竞争一次锁。 
Synchronized 只提供了一种非公平锁的实现。 
3. 从性能方面来看，Synchronized 和 Lock 在性能方面相差不大，在实现上会有一 
些区别，Synchronized 引入了偏向锁、轻量级锁、重量级锁以及锁升级的方式来 
优化加锁的性能，而 Lock 中则用到了自旋锁的方式来实现性能优化

## spring

### bean

- 生命周期

  创建前准备
  创建实例
  依赖注入
  容器缓存
  销毁实例
  
	- 答案

	  1. 创建前准备阶段【干什么、作用】 
	  这个阶段主要的作用是，Bean 在开始加载之前，需要从上下文和相关配置中解析并查 
	  找 Bean 有关的扩展实现， 
	  比如像`init-method`-容器在初始化 bean 时调用的方法、`destory-method`，容器在 
	  销毁 bean 时调用的方法。 
	  以及，BeanFactoryPostProcessor 这类的 bean 加载过程中的前置和后置处理。 
	  这些类或者配置其实是 Spring 提供给开发者，用来实现 Bean 加载过程中的扩展机制， 
	  在很多和 Spring 集成的中间件中比较常见，比如 Dubbo。 
	  
	  2. 创建实例阶段 
	  这个阶段主要是通过反射来创建 Bean 的实例对象，并且扫描和解析 Bean 声明的一些 
	  属性。
	  
	  3. 依赖注入阶段 
	  如果被实例化的 Bean 存在依赖其他 Bean 对象的情况，则需要对这些依赖 bean 进行 
	  对象注入。比如常见的`@Autowired`、setter 注入等依赖注入的配置形式。 
	  同时，在这个阶段会触发一些扩展的调用，比如常见的扩展类：BeanPostProcessors 
	  （用来实现 bean 初始化前后的扩展回调）、 
	  InitializingBean（这个类有一个 afterPropertiesSet()，这个在工作中也比较常见）、 
	  BeanFactoryAware 等等。 
	  4. 容器缓存阶段 
	  容器缓存阶段主要是把 bean 保存到容器以及 Spring 的缓存中，到了这个阶段，Bean 
	  就可以被开发者使用了。 
	  这个阶段涉及到的操作，常见的有，`init-method`这个属性配置的方法， 会在这个阶 
	  段调用。 
	  以及像 BeanPostProcessors 方法中的后置处理器方法如： 
	  postProcessAfterInitialization，也会在这个阶段触发。 
	  
	  5. 销毁实例阶段 
	  当 Spring 应用上下文关闭时，该上下文中的所有 bean 都会被销毁。 
	  如果存在 Bean 实现了 DisposableBean 接口，或者配置了`destory-method`属性，会 
	  在这个阶段被调用
	  
- 作用域

  Singleton 也就是单例，意味着在整个Spring容器中只会存在一个Bean实例
      Prototype 翻译成原型，意味着每次从IOC容器中获取指定Bean的时候，都会返回一个新的实例对象
      但是在给予Spring框架下的Web应用 增加了一个会话纬度 去控制Bean的生命周期
      request 针对每一个http请求 都会创建一个新的Bean
      session 以session会话为纬度 同一个session共享一个Bean实例 不同的session产生不同的bean实例
      globalSession 针对全局session纬度 共享一个Bean实例
  
  
- 注入方式

  1 set法注入
  2 构造方法注入
  3 静态工厂注入
  4 实例工厂注入
  
- [bean是线程安全的吗](https://blog.csdn.net/han_jr123/article/details/118313079)

### spring循环依赖

在代码中两个或多个bean互相持有对方的引用，就会发生循环依赖
   循环依赖三种形态
   1、A依赖B B依赖A
   2、A依赖B B依赖C C依赖A 三者之间形成循环依赖
   3、A依赖A 自我依赖
   Spring设计了三级缓存去解决循环依赖的问题 一级缓存放所有的成熟Bean 二级缓存存放所有的早期Bean 三级缓存代理的Bean
   四种情况下的循环依赖是没法被解决的
   1、多实例Bean通过setter注入的时候 
   2、构造器注入
   3、单例的代理Bean通过setter注入
   4、设置@DependsOn注解的情况下 

### [beanfacotry，factorybean，applicationcontext](https://blog.csdn.net/J080624/article/details/53573211)

### 核心组件。。。

### aop切面

- 通知类型

  @before @after @afterThrowing @afterReturn @around
  
### 介绍下spring怎么使用设计模式的

单例模式，建造者模式，工厂模式，策略模式，观察者模式，装饰者模式。(结合项目说明下自己使用的设计模式)
   结合spring说说
   工厂模式：使用BeanFactory和ApplicationContext来创建对象。 引申beanfactory和factoryBean的区别
   单例模式：bean对象默认是单例的
   策略模式：例如Recourse的实现类，针对不同的资源文件，实现了不同的资源获取策略
   代理模式：aop功能用到了java的动态代理和cglib技术
   适配器模式：aop的增强和advice通知用到了适配器模式

### 过滤器和拦截器

运行顺序不同：过滤器是在 Servlet 容器接收到请求之后，但在 Servlet 被调用之前运行的；而拦截器则是在 Servlet 被调用之后，但在响应被发送到客户 端之前运行的。 
2. 配置方式不同：过滤器是在 web.xml 中进行配置；而拦截器的配置则是在 Spring 
的配置文件中进行配置，或者使用注解进行配置。 
3. Filter 依赖于 Servlet 容器，而 Interceptor 不依赖于 Servlet 容器 
4. Filter 在过滤是只能对 request 和 response 进行操作，而 interceptor 可以对 
request、response、handler、modelAndView、exception 进行操作。

### [统一异常处理](https://www.jianshu.com/p/5b20456ea579)

### spring中几种类加载器

### [对spring的理解 ioc/aop](https://blog.csdn.net/cxydkh/article/details/108659543)

### [ioc 的流程](https://blog.csdn.net/qq_31552775/article/details/125380651)

### spring事务实现方式

Spring事务底层是基于数据库事务和AOP机制实现的
首先对于使用了@Transactional注解的Bean，Spring会创建一个代理对象作为Bean
当调用代理对象的方法时，会先判断该方法上是否加了@Transactinal的注解
如果加了 那么则利用事务管理器创建一个数据库连接
并且修改数据库连接的autocommit属性为false 禁止此连接的自动提交 这是实现Spring事务非常重要的一步

### springMVC执行流程

Spring MVC 的工作流程可以分为几个步骤 
1. 用户发起请求，请求先被 Servlet 拦截转发给 Spring MVC 框架 
2. Spring MVC 里面的 DispatcherSerlvet 核心控制器，会接收到请求并转发给 
HandlerMapping 
3. HandlerMapping 负责解析请求，根据请求信息和配置信息找到匹配的 Controller 
类，不过这里如果有配置拦截器，就会按照顺序执行拦截器里面的 preHandle 方 
法 
4. 找到匹配的 Controller 以后，把请求参数传递给 Controller 里面的方法 
5. Controller 中的方法执行完以后，会返回一个 ModeAndView，这里面会包括视 
图名称和需要传递给视图的模型数据 
6. 视图解析器根据名称找到视图，然后把数据模型填充到视图里面再渲染成 Html 内 
容返回给客户端

### [spring事务失效的几种场景](https://blog.csdn.net/jiahao1186/article/details/122484466)

### [自定义注解](https://blog.csdn.net/qq_34874784/article/details/131092094)

## springboot

### 优点

1 快速构建spring应用
2 直接嵌入tomcat等服务器，无须打成war包
3 通过starter简化构建配置
4 自动化配置spring和第三方库
5 极少的代码生成和xml配置

### 常用注解

- [一般会引申@SpringBootApplication底层实现](https://blog.csdn.net/cheng__yu/article/details/115558200)

### [自定义starter方案](https://blog.csdn.net/qq_38628046/article/details/121665621)

### 配置文件加载顺序

命令行参数
来自 java:comp/env 的 JNDI 属性
Java 系统属性（System.getProperties()）
操作系统环境变量
RandomValuePropertySource 配置的 random.* 属性值
配置文件（YAML文件、Properties 文件）
@Configuration 注解类上的 @PropertySource 指定的配置文件
通过SpringApplication.setDefaultProperties 指定的默认属性


### 自动装配原理

Spring Boot 在启动时会去 classpath 中中寻找 resources/META-INF/spring.factories 文件；
根据 spring.factories 配置加载 AutoConfigure 类；
根据 @Conditional 注解的条件，进行自动配置并将 Bean 注入 Spring Context。


### [spring，springMVC，springboot的理解](https://blog.csdn.net/qq_35048277/article/details/108749980)

### 对starter的理解

Starter 是 Spring Boot 的四大核心功能特性之一，除此之外，Spring Boot 还有自动 装配、Actuator 监控等特性。 
Spring Boot 里面的这些特性，都是为了让开发者在开发基于 Spring 生态下的企业级 
应用时，只需要关心业务逻辑， 
减少对配置和外部环境的依赖。 
（如图）其中，Starter 是启动依赖，它的主要作用有几个。 
1. Starter 组件以功能为纬度，来维护对应的 jar 包的版本依赖， 
使得开发者可以不需要去关心这些版本冲突这种容易出错的细节。 
2. Starter 组件会把对应功能的所有 jar 包依赖全部导入进来，避免了开发者自己去引 
入依赖带来的麻烦。 
3. Starter 内部集成了自动装配的机制，也就说在程序中依赖对应的starter 组件以后， 
这个组件自动会集成到 Spring 生态下，并且对于相关 Bean 的管理，也是基于自动装 
配机制来完成。 
4. 依赖 Starter 组件后，这个组件对应的功能所需要维护的外部化配置，会自动集成 
到 Spring Boot 里面， 
我们只需要在 application.properties 文件里面进行维护就行了，比如 Redis 这个 
starter，只需要在 application.properties 
文件里面添加 redis 的连接信息就可以直接使用了。 
在我看来，Starter 组件几乎完美的体现了 Spring Boot 里面约定优于配置的理念。另外，Spring Boot 官方提供了很多的 Starter 组件，比如 Redis、JPA、MongoDB 
等等。 
但是官方并不一定维护了所有中间件的 Starter，所以对于不存在的 Starter，第三方组 
件一般会自己去维护一个。 
（如图）官方的 starter 和第三方的 starter 组件，最大的区别在于命名上。 
官方维护的 starter 的以 spring-boot-starter 开头的前缀。 
第三方维护的 starter 是以 spring-boot-starter 结尾的后缀 
这也是一种约定优于配置的体现。

## [spring cloud](https://blog.csdn.net/oriettahuiru/article/details/122153659?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-122153659-blog-123780423.235^v38^pc_relevant_anti_vip_base&spm=1001.2101.3001.4242.2&utm_relevant_index=4)

### netfilx

### alibaba

- nacos

	- [注册和配置中心](https://cloud.tencent.com/developer/article/2000200)

- gateway

	- [作用](https://baijiahao.baidu.com/s?id=1761443713370879410&wfr=spider&for=pc)

### [eurka的服务注册原理](https://blog.csdn.net/qq812858143/article/details/104219045)

### [springboot和springcloud的区别](https://blog.csdn.net/2302_77835532/article/details/130466590)

### [zookeeper和eurka的区别](https://blog.csdn.net/qq_40124555/article/details/123318468)

### [说下微服务之间是怎么调用的](https://blog.csdn.net/qq_45632311/article/details/127446415)

基于open fegin RPC）？原理？（http）

### 为什么要熔断降级

### [gateway作用](https://blog.csdn.net/liuwanqing233333/article/details/127981980)

## [dubbo](https://blog.csdn.net/qq_27566167/article/details/130404905)

## redis

### 几种数据结构

- 分别应用场景

  string    缓存 计数器
       List      阻塞队列
       Hash      存储关系型数据表中记录
       Set       统计网站的独立ip
       Zset      带权重的队列 排行榜
  
### redis和mysql数据一致性

主要目的是实现应用和数据库之间的一个读操作的缓存层。主要目的是去减少数据库的io和提升数据的io性能。当应用程序需要去读取某个数据的时候，首先会先去尝试去redis里去加载。如果命中就直接返回，如果没有就从数据库中查询，查询出来后再把数据缓存到redis里。由于数据同时保存在数据库和redis里，当数据发生变化的时候，需要同时去更新数据库和redis
   由于更新操作是有先后顺序的，并不像数据库那样有多表事务操作，可以满足ACID的特性，所以会出现数据一致性的问题。
   解决方案1:先更新数据库，在更新缓存 如果更新缓存失败，就会导致数据库和缓存中数据不一致
   解决方案2:先删除缓存，在更新数据库。由于这两个操作不是原子操作，多线程情况下，还是会出现数据不一致的问题。
   所以在极端情况下 只能采用最终一致性的方案 1 基于RocketMQ的可靠性消息通信

### redis和本地缓存一致性

### 缓存使用

- 一致性

### 热点数据

关于热点 key，简单理解，就是访问频率发很高的 key，由于访问量较大，热点 key 有 
可能会导致服务器资源不足出现宕机的问题。 
热点 key 一般有几种产生的情况： 
 在一些高并发的场景中，比如秒杀、热搜等。 
 由于资源分配不平衡导致访问热点问题 
一般情况下，我们可以通过监控工具、或者客户端程序上报的方式来识别热点 key。 
由于热点 Key 可能存在的影响， 所以我们一般需要去尽可能的规避，从而避免出现性 
能上和稳定性问题，通常的解决方案有以下几种： 
 把热点 key 进行拆分，实现并发流量的分流 
 使用多级缓存的设计，通过增加本地缓存的方式减少目标节点的访问 
 通过对访问频率较高的节点进行扩容，通过负载均衡的方式分散流量

### [redis回收策略、淘汰算法](https://baijiahao.baidu.com/s?id=1765668311166120242&wfr=spider&for=pc)

### 分布式锁

### [泄漏，击穿，雪崩](https://blog.csdn.net/g6U8W7p06dCO99fQ3/article/details/130002390)

- 形成原因

- 解决方案

### 如何处理过期数据

在 redis 中，对于已经过期的数据，Redis 采用两种策略来处理这些数据，分别是惰性删除和定期删除
   惰性删除 惰性删除不会去主动删除数据，而是在访问数据的时候，再检查当前键值是否过期，如果过期则执行删除并返回 null 给客户端，如果没有过期则返回正常信息给客户端。它的优点是简单，不需要对过期的数据做额外的处理，只
   有在每次访问的时候才会检查键值是否过期，缺点是删除过期键不及时，造成了一定的空间浪费。
   定期删除 Redis会周期性的随机测试一批设置了过期时间的key并进行处理。测试到的已过期的key将被删除。
   虽然Redis的确是不断的删除一些过期数据，但是很多没有设置过期时间的数据也会越来越多，那么redis内存不够用的时候是怎么处理的呢？这里我们就会谈到淘汰策略
   常用LRU算法。最近最少使用算法

### 哨兵和集群区别

因为 Redis 集群有两种，一种是主从复制，一种是 Redis Cluster，我不清楚您问的是 哪一种。 
按照我的理解，我认为您可能说的是 Redis 哨兵集群和 Redis Cluster 的区别。 
对于这个问题，我认为可以从 3 个方面来回答 
1. Redis 哨兵集群是基于主从复制来实现的，所以它可以实现读写分离，分担 Redis 读操作的压力 
而 Redis Cluster 集群的 Slave 节点只是实现冷备机制，它只有在 Master 宕机之后才 会工作。 
2. Redis 哨兵集群无法在线扩容，所以它的并发压力受限于单个服务器的资源配置。 
Redis Cluster 提供了基于 Slot 槽的数据分片机制，可以实现在线扩容提升写数据的性 
能 
3. 从集群架构上来说，Redis 哨兵集群是一主多从， 而 Redis Cluster 是多主多从

### 为什么用redis不用本地缓存

1. 数据持久化：Redis可以将数据持久化到磁盘上，这样即使机器宕机或重启，也不会丢失已经保存的数据。而使用本地内存存储没有这样的保障，如果机器出现问题，数据将无法恢复。
2. 高可用性：Redis提供了主从复制机制，可以将数据在多个节点之间进行同步。当主节点宕机时，可以快速自动切换到备用节点，提高了系统的可用性。
3. 分布式：Redis支持分布式部署，可以在多个节点中存储数据。这样即使其中一个节点宕机，也不会影响系统的正常运行。
4. 更好的性能：Redis可以支持更大的内存存储空间，数据存储在内存中可以加快访问速度和响应时间，比本地内存存储更快。
Redis相比于本地内存存储，可以提供更好的数据持久化、高可用性、分布式存储和更好的性能等特性，这使得它成为流行的数据存储解决方案之一。但是使用Redis也需要考虑资源、部署和维护的成本。

### [持久化策略](https://blog.csdn.net/weixin_45295447/article/details/126749087)

## mysql

### [事务的四大特性](https://blog.csdn.net/weixin_43907800/article/details/104571137)

### 隔离级别

### mysql优化

- 硬件和操作系统的优化

- mysql配置优化

- 数据库设计优化

	- 分库分表

	- 读写分离

- sql优化

	- 索引

	- 慢查询定位

### [mvcc](https://blog.csdn.net/lans_g/article/details/124232192)

- 简单介绍

- 实现方式

  就是线程 A 与线程 B 同时进行写操作 
  这种情况下可能会存在数据更新丢失的问题。 
  而 MVCC 就是为了解决事务操作中并发安全性问题的无锁并发控制技术全称为 
  Multi-Version Concurrency Control ，也就是多版本并发控制。它是通过数据库记录 
  中的隐式字段，undo 日志 ，Read View 来实现的。 
  MVCC 主要解决了三个问题 
   第一个是：通过 MVCC 可以解决读写并发阻塞问题从而提升数据并发处理能力 
   第二个是：MVCC 采用了乐观锁的方式实现，降低了死锁的概率 
   第三个是：解决了一致性读的问题也就是事务启动时根据某个条件读取到的数据， 
  直到事务结束时，再次执行相同条件，还是读到同一份数据，不会发生变化。 
  而我们在使用 MVCC 时一般会根据业务场景来选择组合搭配乐观锁或悲观锁。 
  这两个组合中，MVCC 用来解决读写冲突，乐观锁或者悲观锁解决写写冲突从而最大 
  程度的提高数据库并发性能。
  
### [索引](https://blog.csdn.net/guorui_java/article/details/118558095)

- [失效原因](https://zhuanlan.zhihu.com/p/471209432?utm_id=0)

- 分类

### 联合索引

### sql语句

- mysql中count函数，想要null的数据怎么做

  CASE WHEN my_column IS NULL THEN 1 ELSE 0 END
  
  
- [in和exist的区别](https://blog.csdn.net/weixin_43355693/article/details/128825025)

- [where和having的区别](https://blog.csdn.net/weixin_43231592/article/details/107271295)

### [MySQL存储引擎MyISAM与InnoDB区别](https://www.zhihu.com/question/589934906/answer/2940859380)

### [BINLOG和REDO/UNDO LOG的区别](https://www.cnblogs.com/gaogao67/p/10384751.html)

### 和postgresql的区别

PG和MySQL的区别：1、PG支持更多的数据类型，而MySQL不支持JSON和XML数据类型等；2、PG具有更高的扩展性，而MySQL的扩展性较差；3、PG只支持单个存储引擎，而MySQL支持多个存储引擎；4、PG更适合复杂的查询和高并发的情况，而在特定的使用场景下，MySQL的性能可能更好等等。PG和MySQL都是出色的关系型数据库管理系统，跟据具体的需求和使用场景去选择。

- [链接](https://www.php.cn/faq/559985.html)

### [和oracle的区别](https://blog.csdn.net/weixin_31682031/article/details/116542555)

### [分库分表的原则](https://www.yzktw.com.cn/post/932865.html)

### mysql索引的结构，特点

### B+tree和Hash索引的区别？

（1）Hash索引不能进行范围查询，而B+树可以。
这是因为Hash索引指向的数据是无序的，而B+ 树的叶子节点是个有序的链表。

（2）Hash索引不支持联合索引的最左侧原则（即联合索引的部分索引无法使用），而B+树可以。
对于联合索引来说，Hash索引在计算Hash值的时候是将索引键合并后再一起计算Hash值，所以不会针对每个索引单独计算Hash值。因此如果用到联合索引的一个或多个索引时，联合索引无法被利用。

（3）Hash索引不支持Order BY排序，而B+树支持。
因为Hash索引指向的数据是无序的，因此无法起到排序优化的作用，而B+树索引数据是有序的，可以起到对该字段Order By 排序优化的作用。

（4）Hash索引无法进行模糊查询。而B+ 树使用 LIKE 进行模糊查询的时候，LIKE后面前模糊查询（比如%开头）的话可以起到优化的作用。

（5）Hash索引在等值查询上比B+树效率更高。
不过也存在一种情况，就是索引列的重复值如果很多，效率就会降低。这是因为遇到Hash冲突时，需要遍历桶中的行指针来进行比较，找到查询的关键字非常耗时。所以Hash索引通常不会用到重复值多的列上，比如列为性别，年龄等。

### 聚焦索引和非聚焦索引

简单来说，聚集索引就是基于主键创建的索引，除了主键索引以外的其他索引，称 
为非聚集索引，也叫做二级索引。 
53. 由于在 InnoDB 引擎里面，一张表的数据对应的物理文件本身就是按照 B+树来组 
织的一种索引结构，而聚集索引就是按照每张表的主键来构建一颗 B+树，然后叶 
子节点里面存储了这个表的每一行数据记录。 
54. 所以基于 InnoDB 这样的特性，聚集索引并不仅仅是一种索引类型，还代表着一种 
数据的存储方式。55. 同时也意味着每个表里面必须要有一个主键，如果没有主键，InnoDB 会默认选择 
或者添加一个隐藏列作为主键索引来存储这个表的数据行。一般情况是建议使用自 
增 id 作为主键，这样的话 id 本身具有连续性使得对应的数据也会按照顺序存储在 
磁盘上，写入性能和检索性能都很高。否则，如果使用 uuid 这种随机 id，那么在 
频繁插入数据的时候，就会导致随机磁盘 IO，从而导致性能较低。 
56. 需要注意的是，InnoDB 里面只能存在一个聚集索引，原因很简单，如果存在多个 
聚集索引，那么意味着这个表里面的数据存在多个副本，造成磁盘空间的浪费，以及数据维护的困难

### 如何优化Mysql中的慢查询

利用explain查看sql是否走了索引 如果没有则优化SQL 利用索引
着重看type字段 all表示全表扫描 ref表示走了索引
如果有索引可用 那么检查sql中是否用到了函数 类型转化等导致索引失效
如果走了索引还慢 则检查所有字段是不是必须的 是否查询了过多字段 查出了多余数据
是否能通过优化业务逻辑去提高查询速度
检查表中数据是否过多，是否应该进行分库分表了
检查数据库实例所在机器的性能配置 是否太低 是否可以适当增加资源
是否可能通过增加缓存来提升查询速度 比如本地缓存 redis缓存

## mybatis

### 常用sql标签

choose when otherwise
resultType resultMap
where
foreach

### #和$的区别

# 是一个占位符
$ 是一个拼接符
使用#能够防止预注入

### 如何实现分页

1、直接在Select语句上增加数据库提供的分页关键字，然后在代码里传递当前页以及每页展示条数即可
   2、使用Mybatis提供的RowBounds对象，实现内存级别分页
   3、基于Mybatis里面的interceptor拦截器，在select语句执行之前动态拼接分页关键字（Mybatis-plus，pageHelper都是基于interceptor实现的）

### mybatis缓存机制

Mybatis 里面设计的二级缓存是用来提升数据的检索效率，避免每次数据的访 问都需要去查询数据库。 
一级缓存，是 SqlSession 级别的缓存，也叫本地缓存，因为每个用户在执行查询的时候都需要使用 SqlSession 来执行， 
为了避免每次都去查数据库，Mybatis 把查询出来的数据保存到 SqlSession 的本地缓 存中，后续的 SQL 如果命中缓存，就可以直接从本地缓存读取了。 
如果想要实现跨 SqlSession 级别的缓存？那么一级缓存就无法实现了，因此在 Mybatis 里面引入了二级缓存，就是当多个用户在查询数据的时候，只有有任何一个 SqlSession 拿到了数据就会放入到二级缓存里面， 
其他的 SqlSession 就可以从二级缓存加载数据。 
每个 一级缓存的具体实现原理是： 
在 SqlSession 里面持有一个 Executor，每个 Executor 中有一个 LocalCache 对象。 
当用户发起查询的时候，Mybatis 会根据执行语句在 Local Cache 里面查询，如果没命 
中，再去查询数据库并写入到 LocalCache，否则直接返回。 
所以，以及缓存的生命周期是 SqlSessiion，而且在多个 Sqlsession 或者分布式环境下， 
可能会导致数据库写操作出现脏数据。 
（如图）二级缓存的具体实现原理是： 
使用 CachingExecutor 装饰了 Executor，所以在进入一级缓存的查询流程之前，会先 
通过 CachingExecutor 进行二级缓存的查询。开启二级缓存以后，会被多个 SqlSession 共享，所以它是一个全局缓存。因此它的查 
询流程是先查二级缓存，再查一级缓存，最后再查数据库。 
另外，MyBatis 的二级缓存相对于一级缓存来说，实现了 SqlSession 之间缓存数据 
的共享，同时缓存粒度也能够到 namespace 级别，并且还可以通过 Cache 接口实 
现类不同的组合，对 Cache 的可控性也更强

### 为什么不用写dao

dao接口和xml建立关系
初始化sqlsessionFactoryBean
解析mapper.xml 创建sqlSource 创建MapperStatement
Dao接口代理 配置需要扫描的包路径 扫描并注册Bean
使用Bean


## 分布式组件

### [mq](https://blog.csdn.net/qq_42951606/article/details/120757004)

- [对mq的理解](https://blog.csdn.net/qq_45040919/article/details/121131049)

- 顺序发送

      在Kafka里，每个Partition分区的消息本身就是按照顺序存储的。所以只需要针对一个topic设置一个Partition，这样就保证了所有消息都写入到这一个Partition中。而消费者只需要消费这个分区，就可以实现消息的顺序消费处理。
     另外，有些设计方案里面，在消费端会采用异步线程的方式来消费数据去提高消息的处理效率。在这种情况下，因为每个线程的消息处理效率是不同的，所以即便是采用单个分区的存储和消费也可能会出现无序问题，所以针对这个问题的解决方法就是在消费者这边使用一个阻塞队列，把获取到的消息先保存到阻塞队列里面，然后异步线程从阻塞队列里面去获取消息来消费。
  
- 可靠性

  1、消息发布确认机制  kafka支持生产者将消息发布到kafka之后进行确认。生产者发送消息时可以等待相应的确认，确认可以是同步的(即阻塞等待响应)或异步的(即通过回调函数接收响应)，在收到确认之前，生产者会一直
                        重试发送至超时
      2、复制机制  kafka的消息存储方式是分布式的，通过多分备份来保证高可用性。每个分区的数据分布在多个kafka节点上，每个节点都可以作为领导者或副本。生产者将消息发布到主分片上，主分片将所有消息副本同步到所有
                 备份分片，备份分片之间还可以进行数据同步。
      3、消费者提交确认机制 kafka还提供了一种消费者提交确认机制来保证消息被成功消费。消费者可以选择自动或手动确认提交，手动提交需要显示的调用api来进行确认，而自动提交确认则通过一定时间间隔或消息量来确认消费。
     通过以上机制，kafka可以保证消息在生产、存储、消费等环节中的可靠性，并避免消息丢失或重复消费等问题。需要注意的是，消息的可靠性建立在正确的配置和部署之上的，如果配置错误或部署不当，仍然会出现消息丢失等问题。
  
- 消息积压

  如果是因为系统 bug 导致大量消息堆积，那么首先需要解决系统 bug，然后临时做紧 
  急扩容来完成大量消息的消费。 
  1. 首先解决消费端的 bug，来保证消费端的正常消息处理工作。 
  2. 接着把现在所有的消费端停止，然后新建一个 Topic，然后把 Partition 分区数量 
  调整成原来的 10 倍。 
  3. 接着写一个用来实现数据分发的 Consumer 程序，这个程序专门去消费现在积压 
  的数据，消费后不做处理，而是直接再把这些数据写入临时建立的 Topic 的 10 个 
  Partition 中。4. 然后临时增加 10 倍的消费者节点来部署 Consumer，专门来消费临时的 Partition 
  分区数据。 
  通过上面这种方法，可以快速把现在堆积的消息处理完。等积压的消息处理结束后，再 
  把恢复成原来的部署架构，把临时的 Topic 和临时申请的机器释放掉。 
  
- 怎么保证最大吞吐量

- 消息丢失问题

	- 死信队列

	  死信队列（Dead Letter Queue），简称DLQ，是一种用于缓存消息处理异常的队列，通常用于处理那些因为某种原因无法被消费者消费的消息。
	  在Kafka中，死信队列通常是由消费者级别的异常、网络波动、消费者客户端更新、反压（backpressure）等问题导致的消息未能被正常消费。通过将这些消息缓存在死信队列中，可以在其它时间重新恢复和继续处理这些消息，增强了消息消费的容错性和可靠性。
	  
	  Kafka实现死信队列一般有两种方式：
	  
	  1.基于重试机制
	  
	  在这种方式中，如果一个消息处理失败，消费者将会尝试重新消费这条消息，如果还是失败，就将其发送到死信队列中缓存。
	  这种方式的优点是可以尝试减少消息被放到死信队列中的数量，缺点是会增加消费者的压力以及降低整个系统的吞吐量。
	  
	  2.基于时间戳
	  
	  在这种方式中，如果一个消息的时间戳超过预先设定的时间，就将其发送到死信队列中缓存。这个时间戳可以是消息的时间戳，也可以是消费者收到消息的时间戳。
	  这种方式的优点是不会增加消费者的压力以及不会降低整个系统的吞吐量，缺点是可能会标记并缓存那些实际上可以被消费的消息。
	  
- 谈谈你对mq的理解

  异步处理  应用解耦  流量削峰
  
- [rocketmq和kafka的区别](https://blog.csdn.net/shijinghan1126/article/details/104724407)

- [kafka rebelance](https://blog.csdn.net/penriver/article/details/121556161?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-0-121556161-blog-129104668.235^v38^pc_relevant_anti_vip_base&spm=1001.2101.3001.4242.1&utm_relevant_index=1)

### zookeeper

- 分布式锁

  zookeeper 使用有序节点实现分布式锁
                1.每个线程或进程在zookeeper的lock目录下创建一个临时有序的节点去抢占锁，所有创建的节点都会按照先后顺序生成一个带有序编号的节点
                2.线程创建节点后，获取lock节点下的所有子节点，判断当前线程创建的节点是否是所有的节点序号最小的
                3.如果当前线程创建的节点是所有节点序号中最小的，则认为获取锁成功
                4.如果当前创建的节点不是所有节点中序号最小的，则对节点序号的前一个节点添加一个事件监听，当前一个被监听的节点释放锁之后，会触发回调通知，从而再次去抢占锁
  
   优缺点：对于redis的分布式锁说 它获取锁的方式简单粗暴 如果获取不到锁 会不断尝试获取锁 比较耗费性能
                                Redis是ap模型 在集群模式中由于数据一致性会导致锁出现问题，在某些复杂情况下，无法保证百分百可靠性
                                Redis性能很高，在高并发场景下比较合适
             Zookeeper天生设计定位就是分布式协调，强一致性，锁简单易用，适合做分布式锁。如果获取不到锁，只需要添加一个事件监听器即可，性能消耗小。
  
- 监听机制

  Zookeeper 是一个分布式协调组件，为分布式架构下的多个应用组件提供了顺序访问 控制能力。
  它的数据存储采用了类似于文件系统的树形结构，以节点的方式来管理存储在 Zookeeper 上的数据。
  
  Zookeeper 提供了一个 Watch 机制，可以让客户端感知到 Zookeeper Server 上存储的数据变化，这样一种机制可以让 Zookeeper 实现很多的场景，比如配置中心、 注册中心等。
  
  Watch 机制采用了 Push 的方式来实现，也就是说客户端和 Zookeeper Server 会建立 一个长连接，一旦监听的指定节点发生了变化，就会通过这个长连接把变化的事件推送给客户端。
  
  	首先，是客户端通过指定命令比如 exists、get，对特定路径增加 watch 
  	然后服务端收到请求以后，用 HashMap 保存这个客户端会话以及对应关注的节点 路径，同时客户端也会使用 HashMap 存储指定节点和事件回调函数的对应关系。 
   	当服务端指定被 watch 的节点发生变化后，就会找到这个节点对应的会话，把变 化的事件和节点信息发给这个客户端。 
  	客户端收到请求以后，从 ZkWatcherManager 里面对应的回调方法进行调用，完成事件变更的通知。
  
### 怎么处理分布式事务

- [seata](https://blog.csdn.net/m0_69114709/article/details/131095077)

### 配置中心的原理

- [项目用的apollo ](https://blog.csdn.net/lipan121623/article/details/132037631)

### [es](https://blog.csdn.net/BruceLiu_code/article/details/123810268)

## 小知识点

### 深拷贝和浅拷贝

浅拷贝 只是拷贝了原对象的地址 如果原对象的值发生变化 拷贝  对象也发生了变化
深拷贝 创建一个新的数组或者对象 拷贝了源对象的所有值，所以即使源对象的值发生变化时，拷贝对象的值也不会改变。
深拷贝实现方式 构造函数赋值 重载clone()方法 序列化方式

### http协议，rcp协议

从功能特性来说
     http是属于应用层的超文本传输协议，是万维网数据通信的基础，主要服务在网页端和服务端的数据传输上
     rpc是一个远程过程调用协议，它的定位是实现不同计算机应用之间的数据通信，屏蔽通信底层的复杂性，让开发者就像调用本地服务那样完成远程调用
    从实现原来来说
     http协议是一个已经实现且成熟的应用层协议，它定义的通信的报文格式RequestBody和RequestHeader
     Rpc只是一种协议的规范，没有具体的实现，只有按照rpc通信规范实现的通信框架才是rpc的实现，比如dubbo和gRpc，因此，我们可以在实现Rpc框架的时候，自定义报文特性的协议规范，自定义序列化方式，自定义网络
     通信协议的类型等。
     因此，从这个层面来说，http是成熟的应用协议，而rpc只是定义不同服务之间的通信规范
    最后，应用层面来说
     http协议和实现了rpc协议的框架都能够实现跨网络节点的服务之间的通信，并且他们底层都是使用Tcp协议作为通信基础
     但是，由于rpc只是一种标准协议，只要符合rpc协议的框架都属于rpc框架。因此，rpc之间的网络通信层也可以使用http协议来实现。比如gRpc，OpenFeign底层都采用http协议来实现

### 底层崩溃 查询关键字

### [==和equals的区别](https://blog.csdn.net/m0_64813329/article/details/124974157)

### [int和Integer的区别](https://www.cnblogs.com/bigdata-stone/p/10560759.html)

### [为什么string是不可变的](https://zhuanlan.zhihu.com/p/463840081)

### [什么是拆箱和装箱？原因是什么](https://www.mdaima.com/it/9267.html)

### [为什么使用BigDecimal](https://www.cnblogs.com/gaopengpy/p/14488985.html)

### 微服务设计原则

1 单一职责原则

2 服务自治原则

3 轻量级通信原则

4 粒度进化原则

## 项目

### 线上问题复盘

### 技术难点攻克

### 上下游、第三方对接

### 程序变慢

- 几种场景

- 怎么排查

### [对熔断的理解，以及如何去设计一个熔断系统](http://www.coloradmin.cn/o/788359.html?action=onClick)

### 分库分表的实现

### 分布式事务的实现

seta

### 查询日志关键字

实时监控日志的变化：tail -f xx.log
按照行号查找：tail -n 100 xx.log
查询某一个日志的区间：cat -n xx.log | tail -n +100 | head -n 100 （查询100行至200行的日志）
按照关键字找日志的信息：cat -n xx.log |grep "debug"
分页查询日志信息：cat -n xx.log | grep "debug" |more
筛选过滤之后，输出到一个文件：cat -n xx.log |grep "debug" > debug.txts


### 应用突然出现OOM异常，怎么排查？

对于还在运行的正常系统
可以使用jmap命令查看JVM各个区域的使用情况
可以通过jstack来查看线程的运行情况，比如哪些线程阻塞 是否出现了死锁
可以通过jstat命令来查看垃圾回收的情况 特别是fullgc 如果发现fullgc比较频繁 那么就需要调优了
对于已经发生了OOM的系统
配置jvm命令 生成dump文件
使用jsisualvm等工具来分析dump文件
根据dump文件找到异常的实例对象和异常的线程 定位到具体的代码
然后在进行详细的分析和调试

### [秒杀场景设计](https://blog.csdn.net/xiangyangsanren/article/details/122753933)

### [设计登录鉴权](http://www.taodudu.cc/news/show-5936848.html?action=onClick)

## [jvm](https://github.com/doocs/jvm)

